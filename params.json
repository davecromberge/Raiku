{"name":"Raiku","tagline":"A non-blocking - Akka IO driven - Riak client for Scala with a cute DSL","body":"# Raiku\r\n\r\n>Petals of the mountain rose\r\n\r\n>Fall now and then,\r\n\r\n>To the sound of the waterfall?\r\n\r\n\r\n## Overview\r\n\r\nRaiku is to Riak as a waterfall is to Akka; a simple Riak client which lets Riak flow to your Scala applications.\r\n\r\nIt's targeted as a non-blocking, performance focused, full-scale alternative to the java Riak driver. \r\n\r\nBased on Akka IO, it uses the iteratee pattern and actors to create the best throughput possible.\r\n\r\n## Status\r\n\r\nThe client should currently treated as a proof of concept, but is stable enough to try out in smaller projects (although the client is used in production in serveral applications).\r\n\r\n**Currently available in the client:**\r\n\r\n* Writing low-level protobuf style read-write objects through a RaikuClient;\r\n* Doing this non-blocking through multiple sockets, handled by a single actor;\r\n* Writing, fetching and deleting single or multiple objects at once;\r\n* Querying items on 2i, based on binary or integral indexes (ranges also supported);\r\n* Sequencing and continuing multiple operations using monad transformers (ValidatedFuture, ValidatedFutureIO);\r\n* Reactive Map/Reduce functionality;\r\n* Auto-Reconnecting client with retrier functionality;\r\n* Naive Reactive bucket for reactive data flows.\r\n\r\n**The following is currently missing in the client, but will be added soon:**\r\n\r\n* Link walking;\r\n* Custom mutations / conflict resolutions;\r\n* Durable mailboxes;\r\n* Least-connection-error-based router / pool;\r\n\r\n## Architecture\r\n\r\nThe client uses Akka IO and iteratees to send and receive protocol buffer encoded data streams over TCP sockets.\r\n\r\nProtocol Buffer messages are transformed into case classes using ScalaBuff, Riak PBC Content classes are serialized into *RWObjects*, which are case classes, containing all information needed to write objects back into Riak.\r\n\r\nYou can use the client to fetch, store and delete these \"low level\" objects, but it's wiser to use the RaikuBucket to store objects converted using a RaikuConverter implementation. \r\n\r\nYou are free to use any value serialisation method available, but I recommended to use the Spray JSON package (behaves very good in multi-threaded environments).\r\n\r\nAll operations return a value in a monad transformer (<code>ValidatedFutureIO</code>) which combines a <code>Validation</code>, <code>Future</code> and <code>IO</code> monad into one type: most (if not all) exceptions will be caught in the validation monad, all async actions are abstracted into a future monad and all IO actions are as pure as possible by using the Scalaz IO monad.\r\n\r\nUse <code>unsafePerformIO</code> to expose the Future, or use <code>unsafeFulFill(d: Duration)</code> to perform IO and wait (blocking) on the future.\r\n\r\n## Usage\r\nUsing the client / bucket is quite simple, check the code of the tests to see all functionality. But it basically comes down to this:\r\n\r\n**Create a client:**\r\n<notextile><pre><code>implicit val system = ActorSystem(\"system\")\r\nval client = RaikuClient(\"localhost\", 8087, 4)\r\n</code></pre></notextile>\r\n\r\n**Create a converter:**\r\n<pre><code>implicit val yFormat = jsonFormat4(Y)\r\n\r\nimplicit val yConverter = new RaikuConverter[Y] {\r\n\tdef read(o: RaikuRWObject): ReadResult[Y] = try {\r\n\t\tyFormat.read(new String(o.value).asJson).success\r\n\t} catch {\r\n\t\tcase e : Throwable => e.failure\r\n\t}\r\n\tdef write(bucket: String, o: Y): RaikuRWObject = RaikuRWObject(bucket, o.id, o.toJson.toString.getBytes, binIndexes = Map(\"group_id\" -> List(o.groupId)), intIndexes = Map(\"age\" -> List(o.age)))\r\n}\r\n</code></pre>\r\n\r\n**Finally, create the bucket:**\r\n<pre><code>val bucket = RaikuBucket[Y](\"raiku_test_y_bucket\", client)\r\n</code></pre>\r\n\r\n## DSL\r\nYou can use the *normal* functions to store, fetch or delete objects:\r\n\r\n<code>fetch</code> / <code>fetchMany</code>\r\n\r\n<code>store</code> / <code>storeMany</code>\r\n\r\n<code>delete</code> / <code>deleteMany</code>\r\n\r\nOr to fetch keys on 2i:\r\n\r\n<code>fetchKeysForBinIndexByValue</code>\r\n\r\n<code>fetchKeysForIntIndexByValue</code>\r\n\r\n<code>fetchKeysForIntIndexByValueRange</code>\r\n\r\nIf you like to take a walk on the wild side, you can try the (currently quite primitive) DSL to do these actions:\r\n\r\n**Fetching objects**\r\n<pre><code>persons ?   personId\r\npersons ?* \tList(personIdA, personIdB)\r\n</code></pre>\r\n\r\n**Storing objects**\r\n<notextile><pre><code>persons <<   Person(\"Basho\", 42, \"Japan\")\r\npersons <<*  List(Person(\"Basho\", 42, \"Japan\"), Person(\"Shiki\", 52, \"Japan\"))\r\n</code></pre></notextile>\r\n\r\n**Deleting objects**\r\n<pre><code>persons - \tPerson(\"Basho\", 42, \"Japan\")\r\npersons -* \t List(Person(\"Basho\", 42, \"Japan\"), Person(\"Shiki\", 52, \"Japan\"))\r\n</code></pre>\r\n\r\n**Querying objects based on 2i**\r\n<pre><code>persons idx \t(\"age\", 42)\r\npersons idx\t (\"country\", \"Japan\")\r\npersons idx\t (\"age\", 39 to 50)\r\n</code></pre>\r\n\r\n## MapReduce\r\nRaiku features full MapReduce support through both a *Reactive API* as through normal `Futures`. \r\n\r\nOne important thing to note is that the MapReduce functionality is run through different actors / sockets then the normal functionality. MapReduce jobs therefor won't consume or block the normal flow of the client.\r\n\r\nMapReduce can be used through a unique DSL, starting with the creation of input for the MapReduce job, for instance (all options can be found in source and tests):\r\n\r\n`MR.bucket(bucketName: String)`: for a bucket based job\r\n\r\n`MR.items(bucketAndKeys: Set[(String, String)])`: for a item based job\r\n\r\n`MR.binIdx(bucketName: String, idx: String, idxv: String)`: for a index based job\r\n\r\n### Phases\r\nAfter creation of the specific input, several functions can be defined to be used as phases within the MapReduce job.\r\n\r\n<notextile><pre><code>val a = BuildInMapFunction(\"Riak.mapValuesJson\")\r\nval b = BuildInMapFunction(\"Riak.filterNotFound\")\r\nval c = BuildInReduceFunction(\"Riak.reduceSum\")\r\nval d = BuildInReduceFunction(\"Riak.reduceMin\")\r\n</code></pre></notextile>\r\n\r\nThe defined input can be used to be injected into the several phases:\r\n<notextile><pre><code>val job = input |>> a >-> b >=> c >-> d\r\n</code></pre></notextile>\r\n\r\nBy Riak default, the last phase is automatically returned. Other phases can be returned by using the `>=>` between two phases (as opposed to the normal `>->`).\r\n\r\nThe run the job, the job is send to the client using the `mapReduce` function.\r\n\r\n<notextile><pre><code>client mapReduce job\r\nres0: Tuple2[List[JsValue], List[JsValue]]\r\n</code></pre></notextile>\r\n\r\nThis enables you to use the MapReduce results in a type safe manner (without run-time checking on the amount of phases). If you want to use the results from a MapReduce job in a Reactive manner, the `streamMapReduce` function can be send to the client:\r\n\r\n<notextile><pre><code>client streamMapReduce job\r\nres0: Tuple2[Enumerator[JsValue], Enumerator[JsValue]]\r\n</code></pre></notextile>\r\n\r\nReturning a `Tuple` of `Play` powered broadcast `Enumerators`, streaming multiple results in a *map phase* and streaming a single result during a *reduce phase*.\r\n\r\n## Reactive API\r\nTo expose further reactive functionality for usage in your favorite reactive web stack, the client is implementing a naive *Reactive API*.\r\n\r\nBecause Riak doens't support bulk inserts, bulk fetches or cursors, the Reactive API won't give you additional performance on top of the multi-fetch & multi-store `Future` implementation. But enables you to compose data flows in a more natural way, leveraging the awesome Reactive API `Play2.0` exposes. \r\n\r\nBecause of the nature of Riak and Iteratees, fetching isn't done in parallel, resulting in (possible) lower performance then the normal API but shouldn't consume additional resources as opposed to the normal functionality. \r\n\r\nThe `RaikuReactiveBucket` exposes the normal `fetch`, `store` and `delete` functionality to be used in combination with `Enumerators` instead of Lists of keys. `Iteratees` are added as end-points for a reactive data flow. `Enumeratees` are implemented to be used in more complex compositions:\r\n\r\n<notextile><pre><code>Enumerator(randomObjects: _*) &> \r\nbucket.storeEnumeratee(returnBody = true) &>\r\nEnumeratee.filter(_.isDefined) &> Enumeratee.map(x ⇒ x.get) &> bucket.deleteEnumeratee() |>>> \r\nIteratee.fold(0) { (result, chunk) ⇒ result + 1 }\r\n</code></pre></notextile>\r\n\r\n## Monadic behavior\r\nYou can use the monadic behavior of <code>ValidatedFutureIO[T]</code> to combine multiple requests:\r\n\r\n<notextile><pre><code>val objs: ValidatedFutureIO[List[Person]] = for {\r\n\tkeys <- persons idx (\"age\", 39 to 50)\r\n\tobjs <- persons ?* keys\r\n} yield objs\r\n</code></pre></notextile>\r\n\r\nOr better, using Scalaz:\r\n\r\n<pre><code>persons idx (\"age\", 39 to 50) >>= ((x: List[String]) => bucket ?* x)</code></pre>\r\n\r\nOr if you want to run queries in parrallel:\r\n\r\n<notextile><pre><code>val storePersons = persons <<* perObjs\r\nval storeCountries = countries <<* countryObjs\r\nValidatedFutureIO.sequence(List(storePersons, storeCountries))</code></pre></notextile>\r\n\r\n\r\n## Credits\r\n\r\nLots of credits go to [Jordan West](https://github.com/jrwest) for the structure of his Scaliak driver,\r\nand to [Sandro Gržičić](https://github.com/SandroGrzicic) for the ScalaBuff implementation.\r\n\r\n## License\r\nCopyright © 2012 Gideon de Kok\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","google":"UA-38816780-1","note":"Don't delete this file! It's used internally to help with page regeneration."}